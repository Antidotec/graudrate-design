{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "961a0ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d3ffdfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19e4f4fb0034422b1a7f79f356d3c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139119\n"
     ]
    }
   ],
   "source": [
    "#构建corpus\n",
    "import os\n",
    "path =\"D:/毕业设计/data/news_abstract/\"  # 待处理的数据\n",
    "fileList = os.listdir(path)\n",
    "docs = [] #文档集合\n",
    "for i in tqdm(range(len(fileList))):\n",
    "    file = fileList[i]\n",
    "    filePath = os.path.join(path, file)\n",
    "    f = open(filePath, encoding='utf-8')\n",
    "    content = f.read()\n",
    "    f.close()\n",
    "    docs.append(content)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef7026ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "D:\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f605f1b5f7f4d7c854037fcf42c5975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def Preprocessing(text):\n",
    "    # 将文本转成小写\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 删除我们的标点符号\n",
    "    for c in string.punctuation:\n",
    "        text = text.replace(c,\" \")\n",
    "        \n",
    "    # 分词\n",
    "    wordList = tokenizer.tokenize(text)\n",
    "    \n",
    "    # 去除停顿词\n",
    "    filtered = [w for w in wordList if w not in stopwords.words('english')]\n",
    "\n",
    "#     # stem \n",
    "#     ps = PorterStemmer()\n",
    "#     filtered = [ps.stem(w) for w in filtered]\n",
    "    \n",
    "    # 词形还原\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    filtered = [wordnet_lemmatizer.lemmatize(w) for w in filtered ]\n",
    "    \n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    filtered = [token for token in filtered if not token.isnumeric()]\n",
    "    \n",
    "    # Remove words that are only one character.\n",
    "    filtered = [token for token in filtered if len(token) > 1]\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "for idx in tqdm(range(len(docs))):\n",
    "    docs[idx] = Preprocessing(docs[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d6200f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-08 13:07:25,283 : INFO : collecting all words and their counts\n",
      "2022-05-08 13:07:25,283 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2022-05-08 13:07:26,208 : INFO : PROGRESS: at sentence #10000, processed 526415 words and 338228 word types\n",
      "2022-05-08 13:07:27,190 : INFO : PROGRESS: at sentence #20000, processed 1049374 words and 585196 word types\n",
      "2022-05-08 13:07:28,263 : INFO : PROGRESS: at sentence #30000, processed 1576489 words and 800600 word types\n",
      "2022-05-08 13:07:29,165 : INFO : PROGRESS: at sentence #40000, processed 2102844 words and 996469 word types\n",
      "2022-05-08 13:07:30,049 : INFO : PROGRESS: at sentence #50000, processed 2630374 words and 1176505 word types\n",
      "2022-05-08 13:07:30,944 : INFO : PROGRESS: at sentence #60000, processed 3156358 words and 1347368 word types\n",
      "2022-05-08 13:07:31,888 : INFO : PROGRESS: at sentence #70000, processed 3681923 words and 1505667 word types\n",
      "2022-05-08 13:07:32,769 : INFO : PROGRESS: at sentence #80000, processed 4209952 words and 1657250 word types\n",
      "2022-05-08 13:07:33,736 : INFO : PROGRESS: at sentence #90000, processed 4740132 words and 1807473 word types\n",
      "2022-05-08 13:07:34,712 : INFO : PROGRESS: at sentence #100000, processed 5268940 words and 1949445 word types\n",
      "2022-05-08 13:07:35,694 : INFO : PROGRESS: at sentence #110000, processed 5794402 words and 2085067 word types\n",
      "2022-05-08 13:07:36,684 : INFO : PROGRESS: at sentence #120000, processed 6319479 words and 2215658 word types\n",
      "2022-05-08 13:07:37,697 : INFO : PROGRESS: at sentence #130000, processed 6845826 words and 2339413 word types\n",
      "2022-05-08 13:07:38,700 : INFO : collected 2449536 word types from a corpus of 7325007 words (unigram + bigrams) and 139119 sentences\n",
      "2022-05-08 13:07:38,700 : INFO : using 2449536 counts as vocab in Phrases<0 vocab, min_count=20, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a98619ab5347e59a4ada1d48cc9aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in tqdm(range(len(docs))):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ed39590",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-08 13:19:22,323 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2022-05-08 13:19:23,194 : INFO : adding document #10000 to Dictionary(35922 unique tokens: ['access', 'accidental', 'accidental_damage', 'apple', 'applecare']...)\n",
      "2022-05-08 13:19:24,733 : INFO : adding document #20000 to Dictionary(47341 unique tokens: ['access', 'accidental', 'accidental_damage', 'apple', 'applecare']...)\n",
      "2022-05-08 13:19:25,612 : INFO : adding document #30000 to Dictionary(55553 unique tokens: ['access', 'accidental', 'accidental_damage', 'apple', 'applecare']...)\n",
      "2022-05-08 13:19:26,604 : INFO : adding document #40000 to Dictionary(62351 unique tokens: ['access', 'accidental', 'accidental_damage', 'apple', 'applecare']...)\n",
      "2022-05-08 13:19:27,554 : INFO : adding document #50000 to Dictionary(68020 unique tokens: ['access', 'accidental', 'accidental_damage', 'apple', 'applecare']...)\n",
      "2022-05-08 13:19:28,612 : INFO : adding document #60000 to Dictionary(73418 unique tokens: ['access', 'accidental', 'accidental_damage', 'apple', 'applecare']...)\n",
      "2022-05-08 13:19:29,651 : INFO : adding document #70000 to Dictionary(77996 unique tokens: ['access', 'accidental', 'accidental_damage', 'apple', 'applecare']...)\n",
      "2022-05-08 13:19:30,635 : INFO : adding document #80000 to Dictionary(82251 unique tokens: ['access', 'accidental', 'accidental_damage', 'apple', 'applecare']...)\n",
      "2022-05-08 13:19:31,595 : INFO : adding document #90000 to Dictionary(86929 unique tokens: ['access', 'accidental', 'accidental_damage', 'apple', 'applecare']...)\n",
      "2022-05-08 13:19:32,613 : INFO : adding document #100000 to Dictionary(91058 unique tokens: ['access', 'accidental', 'accidental_damage', 'apple', 'applecare']...)\n",
      "2022-05-08 13:19:33,583 : INFO : adding document #110000 to Dictionary(94713 unique tokens: ['access', 'accidental', 'accidental_damage', 'apple', 'applecare']...)\n",
      "2022-05-08 13:19:34,530 : INFO : adding document #120000 to Dictionary(98604 unique tokens: ['access', 'accidental', 'accidental_damage', 'apple', 'applecare']...)\n",
      "2022-05-08 13:19:35,523 : INFO : adding document #130000 to Dictionary(101746 unique tokens: ['access', 'accidental', 'accidental_damage', 'apple', 'applecare']...)\n",
      "2022-05-08 13:19:36,398 : INFO : built Dictionary(104587 unique tokens: ['access', 'accidental', 'accidental_damage', 'apple', 'applecare']...) from 139119 documents (total 8378531 corpus positions)\n",
      "2022-05-08 13:19:36,609 : INFO : discarding 82103 tokens: [('17pm', 3), ('penis', 19), ('shephard', 19), ('trended', 7), ('diameter', 15), ('43f', 6), ('budrul', 6), ('chukrut', 6), ('2020lead', 4), ('2020misleading', 2)]...\n",
      "2022-05-08 13:19:36,609 : INFO : keeping 22484 tokens which were in no less than 20 and no more than 69559 (=50.0%) documents\n",
      "2022-05-08 13:19:36,662 : INFO : resulting dictionary: Dictionary(22484 unique tokens: ['access', 'accidental', 'accidental_damage', 'apple', 'applecare']...)\n"
     ]
    }
   ],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d760e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62971ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 22484\n",
      "Number of documents: 139119\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20e2f862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-08 13:40:59,574 : INFO : serializing temporary corpus to C:\\Users\\huiye\\AppData\\Local\\Temp\\511e37_corpus.txt\n",
      "2022-05-08 13:41:07,602 : INFO : converting temporary corpus to MALLET format with D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input C:\\Users\\huiye\\AppData\\Local\\Temp\\511e37_corpus.txt --output C:\\Users\\huiye\\AppData\\Local\\Temp\\511e37_corpus.mallet\n",
      "2022-05-08 13:41:33,160 : INFO : training MALLET LDA with D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet train-topics --input C:\\Users\\huiye\\AppData\\Local\\Temp\\511e37_corpus.mallet --num-topics 22  --alpha 50 --optimize-interval 0 --num-threads 4 --output-state C:\\Users\\huiye\\AppData\\Local\\Temp\\511e37_state.mallet.gz --output-doc-topics C:\\Users\\huiye\\AppData\\Local\\Temp\\511e37_doctopics.txt --output-topic-keys C:\\Users\\huiye\\AppData\\Local\\Temp\\511e37_topickeys.txt --num-iterations 1000 --inferencer-filename C:\\Users\\huiye\\AppData\\Local\\Temp\\511e37_inferencer.mallet --doc-topics-threshold 0.0  --random-seed 0\n",
      "2022-05-08 13:47:43,675 : INFO : loading assigned topics from C:\\Users\\huiye\\AppData\\Local\\Temp\\511e37_state.mallet.gz\n"
     ]
    }
   ],
   "source": [
    "#无用了\n",
    "import gensim\n",
    "import time\n",
    "# Set training parameters.\n",
    "num_topics = 22\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "os.environ.update({'MALLET_HOME': r'D:/毕业设计/dependencies/mallet-2.0.8'})\n",
    "mallet_path = 'D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet' # update this path\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word = dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab60c06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 19:00:51,199 : INFO : serializing temporary corpus to C:\\Users\\huiye\\AppData\\Local\\Temp\\e69ab7_corpus.txt\n",
      "2022-04-25 19:00:58,218 : INFO : converting temporary corpus to MALLET format with D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input C:\\Users\\huiye\\AppData\\Local\\Temp\\e69ab7_corpus.txt --output C:\\Users\\huiye\\AppData\\Local\\Temp\\e69ab7_corpus.mallet\n",
      "2022-04-25 19:01:16,459 : INFO : training MALLET LDA with D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet train-topics --input C:\\Users\\huiye\\AppData\\Local\\Temp\\e69ab7_corpus.mallet --num-topics 4  --alpha 50 --optimize-interval 0 --num-threads 4 --output-state C:\\Users\\huiye\\AppData\\Local\\Temp\\e69ab7_state.mallet.gz --output-doc-topics C:\\Users\\huiye\\AppData\\Local\\Temp\\e69ab7_doctopics.txt --output-topic-keys C:\\Users\\huiye\\AppData\\Local\\Temp\\e69ab7_topickeys.txt --num-iterations 1000 --inferencer-filename C:\\Users\\huiye\\AppData\\Local\\Temp\\e69ab7_inferencer.mallet --doc-topics-threshold 0.0  --random-seed 0\n",
      "2022-04-25 19:06:00,152 : INFO : loading assigned topics from C:\\Users\\huiye\\AppData\\Local\\Temp\\e69ab7_state.mallet.gz\n",
      "2022-04-25 19:06:18,854 : INFO : using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows\n",
      "2022-04-25 19:06:39,383 : INFO : 7 accumulators retrieved from output queue\n",
      "2022-04-25 19:06:39,595 : INFO : accumulated word occurrence stats for 310811 virtual documents\n",
      "2022-04-25 19:06:39,751 : INFO : serializing temporary corpus to C:\\Users\\huiye\\AppData\\Local\\Temp\\810158_corpus.txt\n",
      "2022-04-25 19:06:46,597 : INFO : converting temporary corpus to MALLET format with D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input C:\\Users\\huiye\\AppData\\Local\\Temp\\810158_corpus.txt --output C:\\Users\\huiye\\AppData\\Local\\Temp\\810158_corpus.mallet\n",
      "2022-04-25 19:07:04,399 : INFO : training MALLET LDA with D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet train-topics --input C:\\Users\\huiye\\AppData\\Local\\Temp\\810158_corpus.mallet --num-topics 6  --alpha 50 --optimize-interval 0 --num-threads 4 --output-state C:\\Users\\huiye\\AppData\\Local\\Temp\\810158_state.mallet.gz --output-doc-topics C:\\Users\\huiye\\AppData\\Local\\Temp\\810158_doctopics.txt --output-topic-keys C:\\Users\\huiye\\AppData\\Local\\Temp\\810158_topickeys.txt --num-iterations 1000 --inferencer-filename C:\\Users\\huiye\\AppData\\Local\\Temp\\810158_inferencer.mallet --doc-topics-threshold 0.0  --random-seed 0\n",
      "2022-04-25 19:11:15,375 : INFO : loading assigned topics from C:\\Users\\huiye\\AppData\\Local\\Temp\\810158_state.mallet.gz\n",
      "2022-04-25 19:11:36,943 : INFO : using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows\n",
      "2022-04-25 19:11:55,446 : INFO : 2040 batches submitted to accumulate stats from 130560 documents (-6286007 virtual)\n",
      "2022-04-25 19:11:56,251 : INFO : 7 accumulators retrieved from output queue\n",
      "2022-04-25 19:11:56,284 : INFO : accumulated word occurrence stats for 313321 virtual documents\n",
      "2022-04-25 19:11:56,526 : INFO : serializing temporary corpus to C:\\Users\\huiye\\AppData\\Local\\Temp\\610abc_corpus.txt\n",
      "2022-04-25 19:12:03,642 : INFO : converting temporary corpus to MALLET format with D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input C:\\Users\\huiye\\AppData\\Local\\Temp\\610abc_corpus.txt --output C:\\Users\\huiye\\AppData\\Local\\Temp\\610abc_corpus.mallet\n",
      "2022-04-25 19:12:22,017 : INFO : training MALLET LDA with D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet train-topics --input C:\\Users\\huiye\\AppData\\Local\\Temp\\610abc_corpus.mallet --num-topics 8  --alpha 50 --optimize-interval 0 --num-threads 4 --output-state C:\\Users\\huiye\\AppData\\Local\\Temp\\610abc_state.mallet.gz --output-doc-topics C:\\Users\\huiye\\AppData\\Local\\Temp\\610abc_doctopics.txt --output-topic-keys C:\\Users\\huiye\\AppData\\Local\\Temp\\610abc_topickeys.txt --num-iterations 1000 --inferencer-filename C:\\Users\\huiye\\AppData\\Local\\Temp\\610abc_inferencer.mallet --doc-topics-threshold 0.0  --random-seed 0\n",
      "2022-04-25 19:16:27,487 : INFO : loading assigned topics from C:\\Users\\huiye\\AppData\\Local\\Temp\\610abc_state.mallet.gz\n",
      "2022-04-25 19:16:47,586 : INFO : using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows\n",
      "2022-04-25 19:17:09,307 : INFO : 7 accumulators retrieved from output queue\n",
      "2022-04-25 19:17:09,350 : INFO : accumulated word occurrence stats for 314344 virtual documents\n",
      "2022-04-25 19:17:09,636 : INFO : serializing temporary corpus to C:\\Users\\huiye\\AppData\\Local\\Temp\\811b8d_corpus.txt\n",
      "2022-04-25 19:17:16,833 : INFO : converting temporary corpus to MALLET format with D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input C:\\Users\\huiye\\AppData\\Local\\Temp\\811b8d_corpus.txt --output C:\\Users\\huiye\\AppData\\Local\\Temp\\811b8d_corpus.mallet\n",
      "2022-04-25 19:17:35,241 : INFO : training MALLET LDA with D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet train-topics --input C:\\Users\\huiye\\AppData\\Local\\Temp\\811b8d_corpus.mallet --num-topics 10  --alpha 50 --optimize-interval 0 --num-threads 4 --output-state C:\\Users\\huiye\\AppData\\Local\\Temp\\811b8d_state.mallet.gz --output-doc-topics C:\\Users\\huiye\\AppData\\Local\\Temp\\811b8d_doctopics.txt --output-topic-keys C:\\Users\\huiye\\AppData\\Local\\Temp\\811b8d_topickeys.txt --num-iterations 1000 --inferencer-filename C:\\Users\\huiye\\AppData\\Local\\Temp\\811b8d_inferencer.mallet --doc-topics-threshold 0.0  --random-seed 0\n",
      "2022-04-25 19:21:56,535 : INFO : loading assigned topics from C:\\Users\\huiye\\AppData\\Local\\Temp\\811b8d_state.mallet.gz\n",
      "2022-04-25 19:22:15,889 : INFO : using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows\n",
      "2022-04-25 19:22:41,512 : INFO : 2049 batches submitted to accumulate stats from 131136 documents (-6340285 virtual)\n",
      "2022-04-25 19:22:42,830 : INFO : 7 accumulators retrieved from output queue\n",
      "2022-04-25 19:22:42,872 : INFO : accumulated word occurrence stats for 314109 virtual documents\n",
      "2022-04-25 19:22:43,209 : INFO : serializing temporary corpus to C:\\Users\\huiye\\AppData\\Local\\Temp\\fc6422_corpus.txt\n",
      "2022-04-25 19:22:50,216 : INFO : converting temporary corpus to MALLET format with D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input C:\\Users\\huiye\\AppData\\Local\\Temp\\fc6422_corpus.txt --output C:\\Users\\huiye\\AppData\\Local\\Temp\\fc6422_corpus.mallet\n",
      "2022-04-25 19:23:08,256 : INFO : training MALLET LDA with D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet train-topics --input C:\\Users\\huiye\\AppData\\Local\\Temp\\fc6422_corpus.mallet --num-topics 12  --alpha 50 --optimize-interval 0 --num-threads 4 --output-state C:\\Users\\huiye\\AppData\\Local\\Temp\\fc6422_state.mallet.gz --output-doc-topics C:\\Users\\huiye\\AppData\\Local\\Temp\\fc6422_doctopics.txt --output-topic-keys C:\\Users\\huiye\\AppData\\Local\\Temp\\fc6422_topickeys.txt --num-iterations 1000 --inferencer-filename C:\\Users\\huiye\\AppData\\Local\\Temp\\fc6422_inferencer.mallet --doc-topics-threshold 0.0  --random-seed 0\n",
      "2022-04-25 19:27:36,858 : INFO : loading assigned topics from C:\\Users\\huiye\\AppData\\Local\\Temp\\fc6422_state.mallet.gz\n",
      "2022-04-25 19:27:56,635 : INFO : using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows\n",
      "2022-04-25 19:28:26,043 : INFO : 2056 batches submitted to accumulate stats from 131584 documents (-6373639 virtual)\n",
      "2022-04-25 19:28:27,724 : INFO : 7 accumulators retrieved from output queue\n",
      "2022-04-25 19:28:27,791 : INFO : accumulated word occurrence stats for 314607 virtual documents\n",
      "2022-04-25 19:28:28,198 : INFO : serializing temporary corpus to C:\\Users\\huiye\\AppData\\Local\\Temp\\6be460_corpus.txt\n",
      "2022-04-25 19:28:35,200 : INFO : converting temporary corpus to MALLET format with D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input C:\\Users\\huiye\\AppData\\Local\\Temp\\6be460_corpus.txt --output C:\\Users\\huiye\\AppData\\Local\\Temp\\6be460_corpus.mallet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 19:28:53,424 : INFO : training MALLET LDA with D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet train-topics --input C:\\Users\\huiye\\AppData\\Local\\Temp\\6be460_corpus.mallet --num-topics 14  --alpha 50 --optimize-interval 0 --num-threads 4 --output-state C:\\Users\\huiye\\AppData\\Local\\Temp\\6be460_state.mallet.gz --output-doc-topics C:\\Users\\huiye\\AppData\\Local\\Temp\\6be460_doctopics.txt --output-topic-keys C:\\Users\\huiye\\AppData\\Local\\Temp\\6be460_topickeys.txt --num-iterations 1000 --inferencer-filename C:\\Users\\huiye\\AppData\\Local\\Temp\\6be460_inferencer.mallet --doc-topics-threshold 0.0  --random-seed 0\n",
      "2022-04-25 19:33:38,906 : INFO : loading assigned topics from C:\\Users\\huiye\\AppData\\Local\\Temp\\6be460_state.mallet.gz\n",
      "2022-04-25 19:33:58,989 : INFO : using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows\n",
      "2022-04-25 19:34:32,802 : INFO : 7 accumulators retrieved from output queue\n",
      "2022-04-25 19:34:32,865 : INFO : accumulated word occurrence stats for 314882 virtual documents\n",
      "2022-04-25 19:34:33,334 : INFO : serializing temporary corpus to C:\\Users\\huiye\\AppData\\Local\\Temp\\95296b_corpus.txt\n",
      "2022-04-25 19:34:40,388 : INFO : converting temporary corpus to MALLET format with D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input C:\\Users\\huiye\\AppData\\Local\\Temp\\95296b_corpus.txt --output C:\\Users\\huiye\\AppData\\Local\\Temp\\95296b_corpus.mallet\n",
      "2022-04-25 19:34:58,678 : INFO : training MALLET LDA with D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet train-topics --input C:\\Users\\huiye\\AppData\\Local\\Temp\\95296b_corpus.mallet --num-topics 16  --alpha 50 --optimize-interval 0 --num-threads 4 --output-state C:\\Users\\huiye\\AppData\\Local\\Temp\\95296b_state.mallet.gz --output-doc-topics C:\\Users\\huiye\\AppData\\Local\\Temp\\95296b_doctopics.txt --output-topic-keys C:\\Users\\huiye\\AppData\\Local\\Temp\\95296b_topickeys.txt --num-iterations 1000 --inferencer-filename C:\\Users\\huiye\\AppData\\Local\\Temp\\95296b_inferencer.mallet --doc-topics-threshold 0.0  --random-seed 0\n",
      "2022-04-25 19:40:46,484 : INFO : loading assigned topics from C:\\Users\\huiye\\AppData\\Local\\Temp\\95296b_state.mallet.gz\n",
      "2022-04-25 19:41:06,276 : INFO : using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows\n",
      "2022-04-25 19:41:41,540 : INFO : 2064 batches submitted to accumulate stats from 132096 documents (-6420671 virtual)\n",
      "2022-04-25 19:41:43,432 : INFO : 7 accumulators retrieved from output queue\n",
      "2022-04-25 19:41:43,516 : INFO : accumulated word occurrence stats for 315116 virtual documents\n",
      "2022-04-25 19:41:44,102 : INFO : serializing temporary corpus to C:\\Users\\huiye\\AppData\\Local\\Temp\\5a2fc1_corpus.txt\n",
      "2022-04-25 19:41:51,435 : INFO : converting temporary corpus to MALLET format with D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input C:\\Users\\huiye\\AppData\\Local\\Temp\\5a2fc1_corpus.txt --output C:\\Users\\huiye\\AppData\\Local\\Temp\\5a2fc1_corpus.mallet\n",
      "2022-04-25 19:42:09,803 : INFO : training MALLET LDA with D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet train-topics --input C:\\Users\\huiye\\AppData\\Local\\Temp\\5a2fc1_corpus.mallet --num-topics 18  --alpha 50 --optimize-interval 0 --num-threads 4 --output-state C:\\Users\\huiye\\AppData\\Local\\Temp\\5a2fc1_state.mallet.gz --output-doc-topics C:\\Users\\huiye\\AppData\\Local\\Temp\\5a2fc1_doctopics.txt --output-topic-keys C:\\Users\\huiye\\AppData\\Local\\Temp\\5a2fc1_topickeys.txt --num-iterations 1000 --inferencer-filename C:\\Users\\huiye\\AppData\\Local\\Temp\\5a2fc1_inferencer.mallet --doc-topics-threshold 0.0  --random-seed 0\n",
      "2022-04-25 19:48:02,991 : INFO : loading assigned topics from C:\\Users\\huiye\\AppData\\Local\\Temp\\5a2fc1_state.mallet.gz\n",
      "2022-04-25 19:48:22,951 : INFO : using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows\n",
      "2022-04-25 19:49:04,485 : INFO : 7 accumulators retrieved from output queue\n",
      "2022-04-25 19:49:04,587 : INFO : accumulated word occurrence stats for 315034 virtual documents\n",
      "2022-04-25 19:49:05,231 : INFO : serializing temporary corpus to C:\\Users\\huiye\\AppData\\Local\\Temp\\10a38b_corpus.txt\n",
      "2022-04-25 19:49:12,505 : INFO : converting temporary corpus to MALLET format with D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input C:\\Users\\huiye\\AppData\\Local\\Temp\\10a38b_corpus.txt --output C:\\Users\\huiye\\AppData\\Local\\Temp\\10a38b_corpus.mallet\n",
      "2022-04-25 19:49:31,459 : INFO : training MALLET LDA with D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet train-topics --input C:\\Users\\huiye\\AppData\\Local\\Temp\\10a38b_corpus.mallet --num-topics 20  --alpha 50 --optimize-interval 0 --num-threads 4 --output-state C:\\Users\\huiye\\AppData\\Local\\Temp\\10a38b_state.mallet.gz --output-doc-topics C:\\Users\\huiye\\AppData\\Local\\Temp\\10a38b_doctopics.txt --output-topic-keys C:\\Users\\huiye\\AppData\\Local\\Temp\\10a38b_topickeys.txt --num-iterations 1000 --inferencer-filename C:\\Users\\huiye\\AppData\\Local\\Temp\\10a38b_inferencer.mallet --doc-topics-threshold 0.0  --random-seed 0\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "num_topics = 4\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "os.environ.update({'MALLET_HOME': r'D:/毕业设计/dependencies/mallet-2.0.8'})\n",
    "mallet_path = 'D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet' # update this path\n",
    "#迭代的方法求最优的主题数量\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=docs, start=4, limit=30, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7d0bebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4974949980606632, 0.5073471509466166, 0.550571053964901, 0.6050765517322579, 0.6121855726104608, 0.5802254489659047, 0.597558919486616, 0.6405275092466353, 0.6075123605162159, 0.650421151768174, 0.6476047200984949, 0.6388919373577777, 0.6195702750971473]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#绘制coherence_values—n_topics图像\n",
    "import matplotlib.pyplot as plt\n",
    "print(coherence_values)\n",
    "limit=30; start=4; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaac3e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-25 20:34:36,580 : INFO : NumExpr defaulting to 8 threads.\n",
      "2022-04-25 20:34:39,092 : INFO : using serial LDA version on this node\n",
      "D:\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:247: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  by='saliency', ascending=False).head(R).drop('saliency', 1)\n"
     ]
    }
   ],
   "source": [
    "#生成 html 图像\n",
    "import pyLDAvis.gensim_models\n",
    "from gensim.models.wrappers.ldamallet import malletmodel2ldamodel\n",
    "#将两个模型进行住转化\n",
    "vis_model = malletmodel2ldamodel(model_list[9])\n",
    "vis = pyLDAvis.gensim_models.prepare(vis_model, corpus, dictionary)\n",
    "pyLDAvis.save_html(vis, 'lda_mallet.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "553b46b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-08 13:53:26,551 : INFO : NumExpr defaulting to 8 threads.\n",
      "2022-05-08 13:53:28,848 : INFO : serializing temporary corpus to C:\\Users\\huiye\\AppData\\Local\\Temp\\511e37_corpus.txt\n",
      "2022-05-08 13:53:36,968 : INFO : converting temporary corpus to MALLET format with D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet import-file --preserve-case --keep-sequence --remove-stopwords --token-regex \"\\S+\" --input C:\\Users\\huiye\\AppData\\Local\\Temp\\511e37_corpus.txt --output C:\\Users\\huiye\\AppData\\Local\\Temp\\511e37_corpus.mallet.infer --use-pipe-from C:\\Users\\huiye\\AppData\\Local\\Temp\\511e37_corpus.mallet\n",
      "2022-05-08 13:54:12,887 : INFO : inferring topics with MALLET LDA 'D:/毕业设计/dependencies/mallet-2.0.8/bin/mallet infer-topics --input C:\\Users\\huiye\\AppData\\Local\\Temp\\511e37_corpus.mallet.infer --inferencer C:\\Users\\huiye\\AppData\\Local\\Temp\\511e37_inferencer.mallet --output-doc-topics C:\\Users\\huiye\\AppData\\Local\\Temp\\511e37_doctopics.txt.infer --num-iterations 100 --doc-topics-threshold 0.0 --random-seed 0'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0934</td>\n",
       "      <td>iphone, apple, pro, ipad, model, mac, macbook,...</td>\n",
       "      <td>[recently, september, worked, apple, launch, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.6442</td>\n",
       "      <td>including, sec, data, information, gov, practi...</td>\n",
       "      <td>[please, declare, traffic, updating, user, age...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.3148</td>\n",
       "      <td>amazon, space, bezos, july, jeff, company, blu...</td>\n",
       "      <td>[shortly, launch, meme, began, trend, across, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.4359</td>\n",
       "      <td>series, show, season, film, tv, star, world, l...</td>\n",
       "      <td>[personal, connection, like, home, run, anti, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2458</td>\n",
       "      <td>iphone, apple, pro, ipad, model, mac, macbook,...</td>\n",
       "      <td>[korean, publication, elec, claim, apple, thre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.1595</td>\n",
       "      <td>share, stock, quarter, market, nasdaq, revenue...</td>\n",
       "      <td>[low, around, 25f, wind, mph, updated, decembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.3533</td>\n",
       "      <td>make, support, page, review, policy, site, ser...</td>\n",
       "      <td>[opt, time, hate, spam, tradersville, net, inv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.1888</td>\n",
       "      <td>apple, watch, apple_watch, pm, event, est, dec...</td>\n",
       "      <td>[wireless, charging, pad, apple, announced, ip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.2756</td>\n",
       "      <td>share, stock, quarter, market, nasdaq, revenue...</td>\n",
       "      <td>[hong, kong, china, american, multinational, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.2429</td>\n",
       "      <td>company, chief, executive, ceo, million, fund,...</td>\n",
       "      <td>[new, orleans, sept, globe, newswire, kahn, sw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0             4.0              0.0934   \n",
       "1            1            14.0              0.6442   \n",
       "2            2            20.0              0.3148   \n",
       "3            3             9.0              0.4359   \n",
       "4            4             4.0              0.2458   \n",
       "5            5            13.0              0.1595   \n",
       "6            6            21.0              0.3533   \n",
       "7            7            17.0              0.1888   \n",
       "8            8            13.0              0.2756   \n",
       "9            9            19.0              0.2429   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  iphone, apple, pro, ipad, model, mac, macbook,...   \n",
       "1  including, sec, data, information, gov, practi...   \n",
       "2  amazon, space, bezos, july, jeff, company, blu...   \n",
       "3  series, show, season, film, tv, star, world, l...   \n",
       "4  iphone, apple, pro, ipad, model, mac, macbook,...   \n",
       "5  share, stock, quarter, market, nasdaq, revenue...   \n",
       "6  make, support, page, review, policy, site, ser...   \n",
       "7  apple, watch, apple_watch, pm, event, est, dec...   \n",
       "8  share, stock, quarter, market, nasdaq, revenue...   \n",
       "9  company, chief, executive, ceo, million, fund,...   \n",
       "\n",
       "                                                Text  \n",
       "0  [recently, september, worked, apple, launch, a...  \n",
       "1  [please, declare, traffic, updating, user, age...  \n",
       "2  [shortly, launch, meme, began, trend, across, ...  \n",
       "3  [personal, connection, like, home, run, anti, ...  \n",
       "4  [korean, publication, elec, claim, apple, thre...  \n",
       "5  [low, around, 25f, wind, mph, updated, decembe...  \n",
       "6  [opt, time, hate, spam, tradersville, net, inv...  \n",
       "7  [wireless, charging, pad, apple, announced, ip...  \n",
       "8  [hong, kong, china, american, multinational, t...  \n",
       "9  [new, orleans, sept, globe, newswire, kahn, sw...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 反过来打标签\n",
    "import pandas as pd\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output \n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    # Get main topic in each document\n",
    "    # ldamodel[doc] => get topic probability distribution for a document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True) # 依据概率分布进行排序\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num) # probability pairs for the most relevant words generated by the topic.\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "optimal_model = ldamallet\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=docs)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69788986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5804</td>\n",
       "      <td>video, game, tv, service, streaming, content, ...</td>\n",
       "      <td>[safe, travel, expert, say, order, jiomart, vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>service, aws, data, cloud, customer, digital, ...</td>\n",
       "      <td>[discover, secure, future, ready, cloud, solut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5382</td>\n",
       "      <td>year, month, customer, time, http, earlier, ba...</td>\n",
       "      <td>[per, monthnew, customer, onlycancel, anytime,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6908</td>\n",
       "      <td>news, day, big, technology, story, tech, worki...</td>\n",
       "      <td>[es, cooky, no, permitem, coletar, alguns, dad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.5720</td>\n",
       "      <td>iphone, apple, pro, ipad, model, mac, macbook,...</td>\n",
       "      <td>[curiously, apple, charging, premium, new, cel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.6273</td>\n",
       "      <td>year, business, company, million, market, bill...</td>\n",
       "      <td>[source, business, insider, source, business, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5197</td>\n",
       "      <td>report, future, ad, browser, enable, enabled, ...</td>\n",
       "      <td>[national, shooting, sport, foundation, gun, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.5673</td>\n",
       "      <td>company, amazon, cook, tech, giant, tim, googl...</td>\n",
       "      <td>[data, protection, bill, changed, fundamentall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.3963</td>\n",
       "      <td>time, thing, people, airbnb, home, good, make,...</td>\n",
       "      <td>[may, seem, like, extreme, approach, fashion, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.6178</td>\n",
       "      <td>series, show, season, film, tv, star, world, l...</td>\n",
       "      <td>[apple, ecstatically, announced, afternoon, la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.5899</td>\n",
       "      <td>chip, amd, rating, based, intel, performance, ...</td>\n",
       "      <td>[xda, official, marketplace, buying, selling, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.5394</td>\n",
       "      <td>deal, amazon, sale, day, black, friday, affili...</td>\n",
       "      <td>[link, top, deal, listed, click, compare, enti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.5995</td>\n",
       "      <td>device, amazon, smart, airpods, home, alexa, w...</td>\n",
       "      <td>[gb, gb, wi, fi, special, offer, without, kind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.6645</td>\n",
       "      <td>share, stock, quarter, market, nasdaq, revenue...</td>\n",
       "      <td>[switch, dark, mode, kinder, eye, night, time,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0.6491</td>\n",
       "      <td>including, sec, data, information, gov, practi...</td>\n",
       "      <td>[please, declare, traffic, updating, user, age...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.6563</td>\n",
       "      <td>price, amazon, today, offering, time, low, hou...</td>\n",
       "      <td>[model, ecovacs, deebot, n8, pro, robot, vacuu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.5402</td>\n",
       "      <td>amazon, store, employee, state, retail, worker...</td>\n",
       "      <td>[see, new, ipados, built, accessibility, featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.9377</td>\n",
       "      <td>apple, watch, apple_watch, pm, event, est, dec...</td>\n",
       "      <td>[dec, pm, est, dec, pm, est, dec, est, dec, es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.7636</td>\n",
       "      <td>app, apple, user, io, update, apps, feature, s...</td>\n",
       "      <td>[microsoft, december, patch, tuesday, fix, zer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.6944</td>\n",
       "      <td>company, chief, executive, ceo, million, fund,...</td>\n",
       "      <td>[mutual, fund, top, tax, saver, fund, better, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.6257</td>\n",
       "      <td>amazon, space, bezos, july, jeff, company, blu...</td>\n",
       "      <td>[register, site, protected, recaptcha, google,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0.5497</td>\n",
       "      <td>make, support, page, review, policy, site, ser...</td>\n",
       "      <td>[post, comment, amazon, resource, susan, ferre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic_Num  Topic_Perc_Contrib  \\\n",
       "0         0.0              0.5804   \n",
       "1         1.0              0.7033   \n",
       "2         2.0              0.5382   \n",
       "3         3.0              0.6908   \n",
       "4         4.0              0.5720   \n",
       "5         5.0              0.6273   \n",
       "6         6.0              0.5197   \n",
       "7         7.0              0.5673   \n",
       "8         8.0              0.3963   \n",
       "9         9.0              0.6178   \n",
       "10       10.0              0.5899   \n",
       "11       11.0              0.5394   \n",
       "12       12.0              0.5995   \n",
       "13       13.0              0.6645   \n",
       "14       14.0              0.6491   \n",
       "15       15.0              0.6563   \n",
       "16       16.0              0.5402   \n",
       "17       17.0              0.9377   \n",
       "18       18.0              0.7636   \n",
       "19       19.0              0.6944   \n",
       "20       20.0              0.6257   \n",
       "21       21.0              0.5497   \n",
       "\n",
       "                                             Keywords  \\\n",
       "0   video, game, tv, service, streaming, content, ...   \n",
       "1   service, aws, data, cloud, customer, digital, ...   \n",
       "2   year, month, customer, time, http, earlier, ba...   \n",
       "3   news, day, big, technology, story, tech, worki...   \n",
       "4   iphone, apple, pro, ipad, model, mac, macbook,...   \n",
       "5   year, business, company, million, market, bill...   \n",
       "6   report, future, ad, browser, enable, enabled, ...   \n",
       "7   company, amazon, cook, tech, giant, tim, googl...   \n",
       "8   time, thing, people, airbnb, home, good, make,...   \n",
       "9   series, show, season, film, tv, star, world, l...   \n",
       "10  chip, amd, rating, based, intel, performance, ...   \n",
       "11  deal, amazon, sale, day, black, friday, affili...   \n",
       "12  device, amazon, smart, airpods, home, alexa, w...   \n",
       "13  share, stock, quarter, market, nasdaq, revenue...   \n",
       "14  including, sec, data, information, gov, practi...   \n",
       "15  price, amazon, today, offering, time, low, hou...   \n",
       "16  amazon, store, employee, state, retail, worker...   \n",
       "17  apple, watch, apple_watch, pm, event, est, dec...   \n",
       "18  app, apple, user, io, update, apps, feature, s...   \n",
       "19  company, chief, executive, ceo, million, fund,...   \n",
       "20  amazon, space, bezos, july, jeff, company, blu...   \n",
       "21  make, support, page, review, policy, site, ser...   \n",
       "\n",
       "                                                 Text  \n",
       "0   [safe, travel, expert, say, order, jiomart, vi...  \n",
       "1   [discover, secure, future, ready, cloud, solut...  \n",
       "2   [per, monthnew, customer, onlycancel, anytime,...  \n",
       "3   [es, cooky, no, permitem, coletar, alguns, dad...  \n",
       "4   [curiously, apple, charging, premium, new, cel...  \n",
       "5   [source, business, insider, source, business, ...  \n",
       "6   [national, shooting, sport, foundation, gun, m...  \n",
       "7   [data, protection, bill, changed, fundamentall...  \n",
       "8   [may, seem, like, extreme, approach, fashion, ...  \n",
       "9   [apple, ecstatically, announced, afternoon, la...  \n",
       "10  [xda, official, marketplace, buying, selling, ...  \n",
       "11  [link, top, deal, listed, click, compare, enti...  \n",
       "12  [gb, gb, wi, fi, special, offer, without, kind...  \n",
       "13  [switch, dark, mode, kinder, eye, night, time,...  \n",
       "14  [please, declare, traffic, updating, user, age...  \n",
       "15  [model, ecovacs, deebot, n8, pro, robot, vacuu...  \n",
       "16  [see, new, ipados, built, accessibility, featu...  \n",
       "17  [dec, pm, est, dec, pm, est, dec, est, dec, es...  \n",
       "18  [microsoft, december, patch, tuesday, fix, zer...  \n",
       "19  [mutual, fund, top, tax, saver, fund, better, ...  \n",
       "20  [register, site, protected, recaptcha, google,...  \n",
       "21  [post, comment, amazon, resource, susan, ferre...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47dfa24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Num_Documents</th>\n",
       "      <th>Perc_Documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>video, game, tv, service, streaming, content, ...</td>\n",
       "      <td>5046</td>\n",
       "      <td>0.0363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>service, aws, data, cloud, customer, digital, ...</td>\n",
       "      <td>7187</td>\n",
       "      <td>0.0517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>year, month, customer, time, http, earlier, ba...</td>\n",
       "      <td>5155</td>\n",
       "      <td>0.0371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>news, day, big, technology, story, tech, worki...</td>\n",
       "      <td>4073</td>\n",
       "      <td>0.0293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>iphone, apple, pro, ipad, model, mac, macbook,...</td>\n",
       "      <td>6797</td>\n",
       "      <td>0.0489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15.0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>year, business, company, million, market, bill...</td>\n",
       "      <td>7852</td>\n",
       "      <td>0.0564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18.0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>report, future, ad, browser, enable, enabled, ...</td>\n",
       "      <td>8809</td>\n",
       "      <td>0.0633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>company, amazon, cook, tech, giant, tim, googl...</td>\n",
       "      <td>7258</td>\n",
       "      <td>0.0522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14.0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>time, thing, people, airbnb, home, good, make,...</td>\n",
       "      <td>2637</td>\n",
       "      <td>0.0190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>series, show, season, film, tv, star, world, l...</td>\n",
       "      <td>3565</td>\n",
       "      <td>0.0256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20.0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>chip, amd, rating, based, intel, performance, ...</td>\n",
       "      <td>5279</td>\n",
       "      <td>0.0379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>deal, amazon, sale, day, black, friday, affili...</td>\n",
       "      <td>6085</td>\n",
       "      <td>0.0437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17.0</th>\n",
       "      <td>12.0</td>\n",
       "      <td>device, amazon, smart, airpods, home, alexa, w...</td>\n",
       "      <td>5181</td>\n",
       "      <td>0.0372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>13.0</td>\n",
       "      <td>share, stock, quarter, market, nasdaq, revenue...</td>\n",
       "      <td>9410</td>\n",
       "      <td>0.0676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>14.0</td>\n",
       "      <td>including, sec, data, information, gov, practi...</td>\n",
       "      <td>8219</td>\n",
       "      <td>0.0591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16.0</th>\n",
       "      <td>15.0</td>\n",
       "      <td>price, amazon, today, offering, time, low, hou...</td>\n",
       "      <td>7419</td>\n",
       "      <td>0.0533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12.0</th>\n",
       "      <td>16.0</td>\n",
       "      <td>amazon, store, employee, state, retail, worker...</td>\n",
       "      <td>8734</td>\n",
       "      <td>0.0628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>17.0</td>\n",
       "      <td>apple, watch, apple_watch, pm, event, est, dec...</td>\n",
       "      <td>4632</td>\n",
       "      <td>0.0333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>app, apple, user, io, update, apps, feature, s...</td>\n",
       "      <td>9879</td>\n",
       "      <td>0.0710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>19.0</td>\n",
       "      <td>company, chief, executive, ceo, million, fund,...</td>\n",
       "      <td>8397</td>\n",
       "      <td>0.0604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>20.0</td>\n",
       "      <td>amazon, space, bezos, july, jeff, company, blu...</td>\n",
       "      <td>2792</td>\n",
       "      <td>0.0201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>21.0</td>\n",
       "      <td>make, support, page, review, policy, site, ser...</td>\n",
       "      <td>4713</td>\n",
       "      <td>0.0339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Dominant_Topic                                     Topic_Keywords  \\\n",
       "21.0             0.0  video, game, tv, service, streaming, content, ...   \n",
       "11.0             1.0  service, aws, data, cloud, customer, digital, ...   \n",
       "19.0             2.0  year, month, customer, time, http, earlier, ba...   \n",
       "10.0             3.0  news, day, big, technology, story, tech, worki...   \n",
       "0.0              4.0  iphone, apple, pro, ipad, model, mac, macbook,...   \n",
       "15.0             5.0  year, business, company, million, market, bill...   \n",
       "18.0             6.0  report, future, ad, browser, enable, enabled, ...   \n",
       "9.0              7.0  company, amazon, cook, tech, giant, tim, googl...   \n",
       "14.0             8.0  time, thing, people, airbnb, home, good, make,...   \n",
       "3.0              9.0  series, show, season, film, tv, star, world, l...   \n",
       "20.0            10.0  chip, amd, rating, based, intel, performance, ...   \n",
       "8.0             11.0  deal, amazon, sale, day, black, friday, affili...   \n",
       "17.0            12.0  device, amazon, smart, airpods, home, alexa, w...   \n",
       "4.0             13.0  share, stock, quarter, market, nasdaq, revenue...   \n",
       "1.0             14.0  including, sec, data, information, gov, practi...   \n",
       "16.0            15.0  price, amazon, today, offering, time, low, hou...   \n",
       "12.0            16.0  amazon, store, employee, state, retail, worker...   \n",
       "6.0             17.0  apple, watch, apple_watch, pm, event, est, dec...   \n",
       "13.0            18.0  app, apple, user, io, update, apps, feature, s...   \n",
       "7.0             19.0  company, chief, executive, ceo, million, fund,...   \n",
       "2.0             20.0  amazon, space, bezos, july, jeff, company, blu...   \n",
       "5.0             21.0  make, support, page, review, policy, site, ser...   \n",
       "\n",
       "      Num_Documents  Perc_Documents  \n",
       "21.0           5046          0.0363  \n",
       "11.0           7187          0.0517  \n",
       "19.0           5155          0.0371  \n",
       "10.0           4073          0.0293  \n",
       "0.0            6797          0.0489  \n",
       "15.0           7852          0.0564  \n",
       "18.0           8809          0.0633  \n",
       "9.0            7258          0.0522  \n",
       "14.0           2637          0.0190  \n",
       "3.0            3565          0.0256  \n",
       "20.0           5279          0.0379  \n",
       "8.0            6085          0.0437  \n",
       "17.0           5181          0.0372  \n",
       "4.0            9410          0.0676  \n",
       "1.0            8219          0.0591  \n",
       "16.0           7419          0.0533  \n",
       "12.0           8734          0.0628  \n",
       "6.0            4632          0.0333  \n",
       "13.0           9879          0.0710  \n",
       "7.0            8397          0.0604  \n",
       "2.0            2792          0.0201  \n",
       "5.0            4713          0.0339  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "df_dominant_topics.sort_values(by=\"Dominant_Topic\", ascending=True, inplace=True)\n",
    "# Show\n",
    "df_dominant_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43272d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
