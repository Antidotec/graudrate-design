{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55e5fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "path = '' # 将要处理的数据\n",
    "\n",
    "def Preprocessing(text):\n",
    "# 将文本转成小写\n",
    "    text = text.lower()\n",
    "# 删除我们的标点符号，\n",
    "    for c in string.punctuation:\n",
    "        text = text.replace(c,\" \")\n",
    "        \n",
    "    text = text.translate(None,'0123456789')\n",
    "# 分词\n",
    "    wordList = nltk.word_tokenize(text)\n",
    "    \n",
    "# 去除停顿词\n",
    "    filtered = [w for w in wordList if w not in stopwords.words('english')]\n",
    "\n",
    "    # stem \n",
    "    ps = PorterStemmer()\n",
    "    filtered = [ps.stem(w) for w in filtered]\n",
    "   # 词形还原\n",
    "   wordnet_lemmatizer = WordNetLemmatizer()\n",
    "   filtered = [wordnet_lemmatizer.lemmatize(w) for w in filtered ]\n",
    "\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib  #也可以选择pickle等保存模型，请随意\n",
    "\n",
    "#构建词汇统计向量并保存，仅运行首次\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(docLst)\n",
    "joblib.dump(tf_vectorizer,tf_ModelPath )\n",
    "\n",
    "\n",
    "def print_top_words(model,feature_names,n_top_words,file,n_topic):\n",
    "    #print the term in each topic with the highest weigh\n",
    "    \n",
    "    filename = file.split(\".\")[0]+\"Topic\"\n",
    "    filePath = os.path.join(outputPath,filename)\n",
    "    fo = open(filePath,\"a\")\n",
    "    line = \"The total number of Topic is: {0}\".format(n_topic) + \"\\n\"\n",
    "    fo.write(line)\n",
    "\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        line1 = \"Topic #{0}\".format(topic_idx) + \"\\n\"\n",
    "        \n",
    "        fo.write(line1)\n",
    "        \n",
    "#        cmd = '''echo \"Topic #\"{0}>>{1}'''.format(topic_idx,filePath)\n",
    "#        os.system(cmd)\n",
    "        line = \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1 : -1]])\n",
    "        fo.write(line)\n",
    "        fo.write(\"\\n\")\n",
    "\n",
    "#        cmd = \"echo {0}>>{1}\".format(line,filePath)\n",
    "#        os.system(cmd)\n",
    "\n",
    "\n",
    "    fo.write(\"******************************\\n\")\n",
    "    fo.close()\n",
    "        \n",
    "\n",
    "\n",
    "def getStarter():\n",
    "\n",
    "    fileList = os.listdir(path)\n",
    "    docList = []\n",
    "    for file in fileList:\n",
    "        filePath = os.path.join(path,file)\n",
    "        f = open(filePath,encoding='utf-8')\n",
    "        content = f.re\n",
    "        f.close()\n",
    "        docLst.append(Preprocessing(content).encode('utf-8'))\n",
    "        \n",
    "    #存储预处理好的文件\n",
    "    with open(textPreFile, 'w') as f:\n",
    "        for line in docLst:\n",
    "            f.write(line+'\\n')  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a6b39c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The move allegedly happened after Apple CEO Tim Cook criticized the social media company. Facebook CEO Mark Zuckerberg. It looks like Mark Zuckerberg can hold a grudge. The Facebook CEO reportedly ordered his management team to start using only Android phones after Apple CEO Tim Cook made public comments about the social media company\\'s Cambridge Analytica data scandal, according to a report Wednesday in The New York Times. \"We\\'re not going to traffic in your personal life,\" Cook said, referencing the scandal in an MSNBC interview in March. \"Privacy to us is a human right. It\\'s a civil liberty.\"\\xa0 After these remarks, the Times reported, Zuckerberg told Facebook executives to use only Android phones because that operating system has higher usage worldwide than Apple\\'s iPhone OS. It isn\\'t clear if Zuckerberg\\'s management team did switch to Android phones. In a blog post Thursday that rebutted points raised in the Times story, Facebook said \"we\\'ve long encouraged our employees and executives to use Android because it is the most popular operating system in the world.\" The Android matter was one of many covered in the Times story, an in-depth look at what the newspaper described as stumbles by Facebook\\'s leadership, including its response to Russian election interference and its handling of fake news on the social network. Facebook came under scrutiny for its privacy practices after The Guardian and The New York Times published accounts of how\\xa0Cambridge Analytica, a digital consultancy firm hired by the Trump presidential campaign, improperly mined personal details from 87 million Facebook users without their permission. Apple, on the other hand, has continually said it\\'s a big proponent of user privacy. Cook has even warned, in multiple interviews, about the dangers of social media and other free online services. In the MSNBC interview, Cook said his company\\xa0purposely chose privacy over profit\\xa0by refusing to sell customer data. \"The truth is, we could make a ton of money if we monetized our customer -- if our customer was our product,\" Cook said. \"We\\'ve elected not to do that.\"  Originally published Nov. 14 at 3:35 p.m. PT.Updated Nov. 15 at 5:49 a.m PT: Added information from Facebook\\'s blog post. CNET\\'s Holiday Gift Guide: The place to find the best tech gifts for 2018. Best Black Friday 2018 deals: The best discounts we\\'ve found so far. Be respectful, keep it civil and stay on topic. We delete comments that violate our policy, which we encourage you to read. Discussion threads can be closed at any time at our discretion.']\n"
     ]
    }
   ],
   "source": [
    "file = open('D:/毕业设计/code/data/Zuckerberg reportedly told Facebook execs to use Android, not Apple, phones', encoding='utf-8').read().splitlines()\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e8435d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_randomized_eigsh' from 'sklearn.utils.extmath' (D:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19424/3499984728.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m  \u001b[1;31m#也可以选择pickle等保存模型，请随意\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D:/毕业设计/code/data/'\u001b[0m  \u001b[1;31m#待处理的数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_pca\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_incremental_pca\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIncrementalPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_kernel_pca\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKernelPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_sparse_pca\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparsePCA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMiniBatchSparsePCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_truncated_svd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_kernel_pca.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_arpack\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_init_arpack_v0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextmath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msvd_flip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_randomized_eigsh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_check_psd_eigenvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_randomized_eigsh' from 'sklearn.utils.extmath' (D:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py)"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import joblib  #也可以选择pickle等保存模型，请随意\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "path = 'D:/毕业设计/code/data/'  #待处理的数据\n",
    "textPreFile = 'D:/毕业设计/code/data_processed'  #预处理好的数据\n",
    "tf_ModelPath = 'D:/毕业设计' #根据doclist形成的tf矩阵\n",
    "n_topics = 5  # 最终要分类的主题个数\n",
    "max_iter = 30 # 迭代轮次\n",
    "\n",
    "def Preprocessing(text):\n",
    "# 将文本转成小写\n",
    "    text = text.lower()\n",
    "# 删除我们的标点符号，\n",
    "    for c in string.punctuation:\n",
    "        text = text.replace(c,\" \")\n",
    "        \n",
    "    text = text.translate(None,'0123456789')\n",
    "# 分词\n",
    "    wordList = nltk.word_tokenize(text)\n",
    "    \n",
    "# 去除停顿词\n",
    "    filtered = [w for w in wordList if w not in stopwords.words('english')]\n",
    "\n",
    "    # stem \n",
    "    ps = PorterStemmer()\n",
    "    filtered = [ps.stem(w) for w in filtered]\n",
    "   # 词形还原\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    filtered = [wordnet_lemmatizer.lemmatize(w) for w in filtered ]\n",
    "\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "\n",
    "fileList = os.listdir(path)\n",
    "docList = []\n",
    "for file in fileList:\n",
    "    filePath = os.path.join(path,file)\n",
    "    f = open(filePath,encoding='utf-8')\n",
    "    content = f.re\n",
    "    f.close()\n",
    "    docLst.append(Preprocessing(content).encode('utf-8'))\n",
    "        \n",
    "#存储预处理好的文件\n",
    "with open(textPreFile, 'w') as f:\n",
    "    for line in docLst:\n",
    "        f.write(line+'\\n')  \n",
    "            \n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(docLst)\n",
    "joblib.dump(tf_vectorizer,tf_ModelPath )\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=n_topic, \n",
    "                                max_iter=50,\n",
    "                                learning_method='batch')\n",
    "lda.fit(tf) #tf即为Document_word Sparse Matrix                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
