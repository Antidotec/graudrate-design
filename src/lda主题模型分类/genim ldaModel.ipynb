{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977c00a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae4dcae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "The move allegedly happened after Apple CEO Tim Cook criticized the social media company. Facebook CEO Mark Zuckerberg. It looks like Mark Zuckerberg can hold a grudge. The Facebook CEO reportedly ordered his management team to start using only Android phones after Apple CEO Tim Cook made public comments about the social media company's Cambridge Analytica data scandal, according to a report Wednesday in The New York Times. \"We're not going to traffic in your personal life,\" Cook said, referenci\n"
     ]
    }
   ],
   "source": [
    "#构建corpus\n",
    "import os\n",
    "\n",
    "path = 'D:/毕业设计/code/data/'  # 待处理的数据\n",
    "\n",
    "fileList = os.listdir(path)\n",
    "docs = [] #文档集合\n",
    "for file in fileList:\n",
    "    filePath = os.path.join(path, file)\n",
    "    f = open(filePath, encoding='utf-8')\n",
    "    content = f.read()\n",
    "    f.close()\n",
    "    docs.append(content)\n",
    "print(len(docs))\n",
    "print(docs[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebb44ce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "D:\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the documents.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d6ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the documents.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6824e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 15:46:24,663 : INFO : collecting all words and their counts\n",
      "2022-04-21 15:46:24,664 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2022-04-21 15:46:24,673 : INFO : collected 6047 word types from a corpus of 6365 words (unigram + bigrams) and 8 sentences\n",
      "2022-04-21 15:46:24,674 : INFO : using 6047 counts as vocab in Phrases<0 vocab, min_count=20, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0851977d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 15:46:24,714 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2022-04-21 15:46:24,721 : INFO : built Dictionary(1644 unique tokens: ['a', 'about', 'according', 'account', 'added']...) from 8 documents (total 6461 corpus positions)\n",
      "2022-04-21 15:46:24,723 : INFO : discarding 1237 tokens: [('a', 7), ('analytica', 1), ('and', 7), ('black', 1), ('blog', 1), ('by', 7), ('cambridge', 1), ('came', 1), ('campaign', 1), ('chose', 1)]...\n",
      "2022-04-21 15:46:24,724 : INFO : keeping 407 tokens which were in no less than 2 and no more than 6 (=80.0%) documents\n",
      "2022-04-21 15:46:24,725 : INFO : resulting dictionary: Dictionary(407 unique tokens: ['about', 'according', 'account', 'added', 'after']...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(407 unique tokens: ['about', 'according', 'account', 'added', 'after']...)\n"
     ]
    }
   ],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.8)\n",
    "print(dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d67feea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dd9ba5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 407\n",
      "Number of documents: 8\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aa70a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 15:46:24,779 : INFO : using autotuned alpha, starting with [0.25, 0.25, 0.25, 0.25]\n",
      "2022-04-21 15:46:24,781 : INFO : using serial LDA version on this node\n",
      "2022-04-21 15:46:24,782 : INFO : running online (multi-pass) LDA training, 4 topics, 20 passes over the supplied corpus of 8 documents, updating model once every 8 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "2022-04-21 15:46:24,782 : INFO : PROGRESS: pass 0, at document #8/8\n",
      "2022-04-21 15:46:24,838 : WARNING : updated prior is not positive\n",
      "2022-04-21 15:46:24,839 : INFO : optimized alpha [0.25, 0.25, 0.25, 0.25]\n",
      "2022-04-21 15:46:24,840 : INFO : topic #0 (0.250): 0.032*\"stock\" + 0.019*\"technology\" + 0.016*\"on\" + 0.015*\"ltd\" + 0.014*\"it\" + 0.014*\"at\" + 0.013*\"share\" + 0.012*\"earnings\" + 0.012*\"inc\" + 0.011*\"that\"\n",
      "2022-04-21 15:46:24,841 : INFO : topic #1 (0.250): 0.034*\"stock\" + 0.028*\"price\" + 0.025*\"ltd\" + 0.025*\"holding\" + 0.022*\"were\" + 0.019*\"share\" + 0.018*\"with\" + 0.018*\"purchase\" + 0.018*\"at\" + 0.017*\"an\"\n",
      "2022-04-21 15:46:24,841 : INFO : topic #2 (0.250): 0.024*\"it\" + 0.019*\"apple\" + 0.015*\"that\" + 0.015*\"we\" + 0.014*\"with\" + 0.014*\"on\" + 0.012*\"facebook\" + 0.012*\"an\" + 0.011*\"ha\" + 0.010*\"from\"\n",
      "2022-04-21 15:46:24,842 : INFO : topic #3 (0.250): 0.003*\"one\" + 0.003*\"canada\" + 0.003*\"uk\" + 0.002*\"number\" + 0.002*\"it\" + 0.002*\"london\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"price\"\n",
      "2022-04-21 15:46:24,842 : INFO : topic diff=1.539740, rho=1.000000\n",
      "2022-04-21 15:46:24,843 : INFO : PROGRESS: pass 1, at document #8/8\n",
      "2022-04-21 15:46:24,851 : WARNING : updated prior is not positive\n",
      "2022-04-21 15:46:24,851 : INFO : optimized alpha [0.25, 0.25, 0.25, 0.25]\n",
      "2022-04-21 15:46:24,852 : INFO : topic #0 (0.250): 0.025*\"stock\" + 0.015*\"quarter\" + 0.013*\"share\" + 0.013*\"technology\" + 0.013*\"on\" + 0.012*\"it\" + 0.012*\"revenue\" + 0.012*\"analyst\" + 0.012*\"at\" + 0.010*\"inc\"\n",
      "2022-04-21 15:46:24,853 : INFO : topic #1 (0.250): 0.038*\"stock\" + 0.029*\"price\" + 0.027*\"ltd\" + 0.025*\"holding\" + 0.022*\"were\" + 0.020*\"share\" + 0.018*\"at\" + 0.018*\"with\" + 0.018*\"inc\" + 0.017*\"purchase\"\n",
      "2022-04-21 15:46:24,853 : INFO : topic #2 (0.250): 0.024*\"it\" + 0.020*\"apple\" + 0.016*\"we\" + 0.015*\"that\" + 0.014*\"on\" + 0.013*\"with\" + 0.013*\"facebook\" + 0.011*\"an\" + 0.011*\"from\" + 0.010*\"ha\"\n",
      "2022-04-21 15:46:24,854 : INFO : topic #3 (0.250): 0.002*\"one\" + 0.002*\"canada\" + 0.002*\"uk\" + 0.002*\"number\" + 0.002*\"london\" + 0.002*\"it\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"price\"\n",
      "2022-04-21 15:46:24,854 : INFO : topic diff=0.225131, rho=0.577350\n",
      "2022-04-21 15:46:24,855 : INFO : PROGRESS: pass 2, at document #8/8\n",
      "2022-04-21 15:46:24,860 : INFO : optimized alpha [0.058523938, 0.15320529, 0.20244384, 0.022951856]\n",
      "2022-04-21 15:46:24,861 : INFO : topic #0 (0.059): 0.022*\"quarter\" + 0.021*\"stock\" + 0.017*\"revenue\" + 0.014*\"analyst\" + 0.014*\"million\" + 0.014*\"share\" + 0.013*\"report\" + 0.012*\"trading\" + 0.011*\"year\" + 0.011*\"market\"\n",
      "2022-04-21 15:46:24,863 : INFO : topic #1 (0.153): 0.040*\"stock\" + 0.029*\"price\" + 0.028*\"ltd\" + 0.025*\"holding\" + 0.022*\"were\" + 0.021*\"share\" + 0.019*\"at\" + 0.018*\"technology\" + 0.018*\"inc\" + 0.018*\"with\"\n",
      "2022-04-21 15:46:24,864 : INFO : topic #2 (0.202): 0.024*\"it\" + 0.021*\"apple\" + 0.016*\"we\" + 0.015*\"that\" + 0.014*\"on\" + 0.013*\"with\" + 0.013*\"facebook\" + 0.011*\"an\" + 0.011*\"from\" + 0.010*\"ha\"\n",
      "2022-04-21 15:46:24,865 : INFO : topic #3 (0.023): 0.002*\"one\" + 0.002*\"uk\" + 0.002*\"london\" + 0.002*\"canada\" + 0.002*\"number\" + 0.002*\"it\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"price\"\n",
      "2022-04-21 15:46:24,866 : INFO : topic diff=0.171171, rho=0.500000\n",
      "2022-04-21 15:46:24,867 : INFO : PROGRESS: pass 3, at document #8/8\n",
      "2022-04-21 15:46:24,871 : INFO : optimized alpha [0.04949488, 0.12117872, 0.16721049, 0.020883892]\n",
      "2022-04-21 15:46:24,871 : INFO : topic #0 (0.049): 0.026*\"quarter\" + 0.020*\"million\" + 0.020*\"revenue\" + 0.019*\"stock\" + 0.016*\"report\" + 0.015*\"share\" + 0.015*\"analyst\" + 0.014*\"trading\" + 0.013*\"get\" + 0.012*\"year\"\n",
      "2022-04-21 15:46:24,872 : INFO : topic #1 (0.121): 0.040*\"stock\" + 0.029*\"price\" + 0.028*\"ltd\" + 0.026*\"holding\" + 0.022*\"were\" + 0.021*\"share\" + 0.019*\"at\" + 0.019*\"technology\" + 0.018*\"inc\" + 0.018*\"purchase\"\n",
      "2022-04-21 15:46:24,873 : INFO : topic #2 (0.167): 0.025*\"it\" + 0.021*\"apple\" + 0.017*\"we\" + 0.015*\"that\" + 0.014*\"on\" + 0.014*\"facebook\" + 0.013*\"with\" + 0.011*\"an\" + 0.011*\"from\" + 0.011*\"ha\"\n",
      "2022-04-21 15:46:24,873 : INFO : topic #3 (0.021): 0.002*\"one\" + 0.002*\"uk\" + 0.002*\"london\" + 0.002*\"canada\" + 0.002*\"number\" + 0.002*\"it\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"price\"\n",
      "2022-04-21 15:46:24,874 : INFO : topic diff=0.137912, rho=0.447214\n",
      "2022-04-21 15:46:24,875 : INFO : PROGRESS: pass 4, at document #8/8\n",
      "2022-04-21 15:46:24,881 : INFO : optimized alpha [0.043565754, 0.10497299, 0.14719115, 0.01924324]\n",
      "2022-04-21 15:46:24,882 : INFO : topic #0 (0.044): 0.027*\"quarter\" + 0.024*\"million\" + 0.020*\"revenue\" + 0.019*\"report\" + 0.018*\"stock\" + 0.016*\"share\" + 0.015*\"analyst\" + 0.014*\"trading\" + 0.014*\"get\" + 0.013*\"year\"\n",
      "2022-04-21 15:46:24,883 : INFO : topic #1 (0.105): 0.041*\"stock\" + 0.030*\"price\" + 0.029*\"ltd\" + 0.026*\"holding\" + 0.023*\"were\" + 0.021*\"share\" + 0.019*\"technology\" + 0.019*\"at\" + 0.018*\"inc\" + 0.018*\"purchase\"\n",
      "2022-04-21 15:46:24,883 : INFO : topic #2 (0.147): 0.025*\"it\" + 0.022*\"apple\" + 0.017*\"we\" + 0.015*\"that\" + 0.014*\"on\" + 0.014*\"facebook\" + 0.013*\"with\" + 0.011*\"an\" + 0.011*\"from\" + 0.011*\"ha\"\n",
      "2022-04-21 15:46:24,884 : INFO : topic #3 (0.019): 0.002*\"one\" + 0.002*\"uk\" + 0.002*\"london\" + 0.002*\"canada\" + 0.002*\"number\" + 0.002*\"it\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"price\"\n",
      "2022-04-21 15:46:24,885 : INFO : topic diff=0.104542, rho=0.408248\n",
      "2022-04-21 15:46:24,886 : INFO : PROGRESS: pass 5, at document #8/8\n",
      "2022-04-21 15:46:24,890 : INFO : optimized alpha [0.039314803, 0.094727464, 0.13386795, 0.017904498]\n",
      "2022-04-21 15:46:24,890 : INFO : topic #0 (0.039): 0.027*\"quarter\" + 0.027*\"million\" + 0.020*\"revenue\" + 0.020*\"report\" + 0.017*\"stock\" + 0.016*\"share\" + 0.015*\"analyst\" + 0.014*\"trading\" + 0.014*\"get\" + 0.013*\"thursday\"\n",
      "2022-04-21 15:46:24,891 : INFO : topic #1 (0.095): 0.041*\"stock\" + 0.030*\"price\" + 0.029*\"ltd\" + 0.026*\"holding\" + 0.023*\"were\" + 0.021*\"share\" + 0.019*\"technology\" + 0.019*\"at\" + 0.018*\"inc\" + 0.018*\"purchase\"\n",
      "2022-04-21 15:46:24,892 : INFO : topic #2 (0.134): 0.025*\"it\" + 0.022*\"apple\" + 0.017*\"we\" + 0.015*\"that\" + 0.014*\"on\" + 0.014*\"facebook\" + 0.013*\"with\" + 0.011*\"an\" + 0.011*\"from\" + 0.011*\"ha\"\n",
      "2022-04-21 15:46:24,892 : INFO : topic #3 (0.018): 0.002*\"one\" + 0.002*\"uk\" + 0.002*\"london\" + 0.002*\"canada\" + 0.002*\"number\" + 0.002*\"it\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"price\"\n",
      "2022-04-21 15:46:24,893 : INFO : topic diff=0.078474, rho=0.377964\n",
      "2022-04-21 15:46:24,894 : INFO : PROGRESS: pass 6, at document #8/8\n",
      "2022-04-21 15:46:24,900 : INFO : optimized alpha [0.036085464, 0.08742456, 0.12415439, 0.01678614]\n",
      "2022-04-21 15:46:24,900 : INFO : topic #0 (0.036): 0.028*\"million\" + 0.027*\"quarter\" + 0.020*\"report\" + 0.020*\"revenue\" + 0.016*\"stock\" + 0.016*\"share\" + 0.014*\"analyst\" + 0.014*\"trading\" + 0.014*\"get\" + 0.013*\"thursday\"\n",
      "2022-04-21 15:46:24,902 : INFO : topic #1 (0.087): 0.041*\"stock\" + 0.030*\"price\" + 0.029*\"ltd\" + 0.026*\"holding\" + 0.023*\"were\" + 0.020*\"share\" + 0.019*\"technology\" + 0.019*\"at\" + 0.018*\"inc\" + 0.018*\"purchase\"\n",
      "2022-04-21 15:46:24,903 : INFO : topic #2 (0.124): 0.025*\"it\" + 0.022*\"apple\" + 0.017*\"we\" + 0.015*\"that\" + 0.014*\"facebook\" + 0.014*\"on\" + 0.013*\"with\" + 0.011*\"an\" + 0.011*\"from\" + 0.011*\"ha\"\n",
      "2022-04-21 15:46:24,903 : INFO : topic #3 (0.017): 0.002*\"one\" + 0.002*\"uk\" + 0.002*\"london\" + 0.002*\"canada\" + 0.002*\"number\" + 0.002*\"it\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"price\"\n",
      "2022-04-21 15:46:24,904 : INFO : topic diff=0.060078, rho=0.353553\n",
      "2022-04-21 15:46:24,905 : INFO : PROGRESS: pass 7, at document #8/8\n",
      "2022-04-21 15:46:24,910 : INFO : optimized alpha [0.033394713, 0.076811895, 0.11506676, 0.015804568]\n",
      "2022-04-21 15:46:24,912 : INFO : topic #0 (0.033): 0.028*\"million\" + 0.026*\"quarter\" + 0.020*\"report\" + 0.019*\"revenue\" + 0.016*\"stock\" + 0.015*\"share\" + 0.014*\"with\" + 0.014*\"analyst\" + 0.014*\"trading\" + 0.013*\"get\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 15:46:24,912 : INFO : topic #1 (0.077): 0.041*\"stock\" + 0.030*\"price\" + 0.029*\"ltd\" + 0.026*\"holding\" + 0.022*\"were\" + 0.020*\"share\" + 0.019*\"technology\" + 0.019*\"at\" + 0.018*\"inc\" + 0.018*\"purchase\"\n",
      "2022-04-21 15:46:24,913 : INFO : topic #2 (0.115): 0.025*\"it\" + 0.022*\"apple\" + 0.017*\"we\" + 0.015*\"that\" + 0.014*\"facebook\" + 0.014*\"on\" + 0.013*\"with\" + 0.011*\"an\" + 0.011*\"from\" + 0.011*\"ha\"\n",
      "2022-04-21 15:46:24,914 : INFO : topic #3 (0.016): 0.002*\"one\" + 0.002*\"uk\" + 0.002*\"london\" + 0.002*\"canada\" + 0.002*\"number\" + 0.002*\"it\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"price\"\n",
      "2022-04-21 15:46:24,915 : INFO : topic diff=0.047841, rho=0.333333\n",
      "2022-04-21 15:46:24,915 : INFO : PROGRESS: pass 8, at document #8/8\n",
      "2022-04-21 15:46:24,921 : INFO : optimized alpha [0.031190353, 0.06950701, 0.10779929, 0.0149525795]\n",
      "2022-04-21 15:46:24,921 : INFO : topic #0 (0.031): 0.028*\"million\" + 0.026*\"quarter\" + 0.020*\"report\" + 0.018*\"revenue\" + 0.016*\"stock\" + 0.015*\"share\" + 0.015*\"with\" + 0.013*\"analyst\" + 0.013*\"trading\" + 0.013*\"get\"\n",
      "2022-04-21 15:46:24,922 : INFO : topic #1 (0.070): 0.041*\"stock\" + 0.030*\"price\" + 0.030*\"ltd\" + 0.026*\"holding\" + 0.022*\"were\" + 0.020*\"share\" + 0.020*\"technology\" + 0.019*\"at\" + 0.018*\"inc\" + 0.018*\"purchase\"\n",
      "2022-04-21 15:46:24,923 : INFO : topic #2 (0.108): 0.025*\"it\" + 0.023*\"apple\" + 0.017*\"we\" + 0.015*\"that\" + 0.014*\"facebook\" + 0.014*\"on\" + 0.013*\"with\" + 0.011*\"an\" + 0.011*\"from\" + 0.011*\"ha\"\n",
      "2022-04-21 15:46:24,923 : INFO : topic #3 (0.015): 0.002*\"one\" + 0.002*\"uk\" + 0.002*\"london\" + 0.002*\"canada\" + 0.002*\"number\" + 0.002*\"it\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"price\"\n",
      "2022-04-21 15:46:24,924 : INFO : topic diff=0.036931, rho=0.316228\n",
      "2022-04-21 15:46:24,925 : INFO : PROGRESS: pass 9, at document #8/8\n",
      "2022-04-21 15:46:24,930 : INFO : optimized alpha [0.029344907, 0.06409532, 0.10179755, 0.014204583]\n",
      "2022-04-21 15:46:24,931 : INFO : topic #0 (0.029): 0.028*\"million\" + 0.025*\"quarter\" + 0.019*\"report\" + 0.018*\"revenue\" + 0.016*\"with\" + 0.015*\"stock\" + 0.015*\"share\" + 0.014*\"it\" + 0.013*\"analyst\" + 0.013*\"trading\"\n",
      "2022-04-21 15:46:24,932 : INFO : topic #1 (0.064): 0.041*\"stock\" + 0.030*\"price\" + 0.030*\"ltd\" + 0.026*\"holding\" + 0.022*\"were\" + 0.020*\"share\" + 0.020*\"technology\" + 0.019*\"at\" + 0.018*\"inc\" + 0.018*\"purchase\"\n",
      "2022-04-21 15:46:24,933 : INFO : topic #2 (0.102): 0.025*\"it\" + 0.023*\"apple\" + 0.017*\"we\" + 0.015*\"that\" + 0.015*\"facebook\" + 0.014*\"on\" + 0.012*\"with\" + 0.011*\"an\" + 0.011*\"from\" + 0.011*\"ha\"\n",
      "2022-04-21 15:46:24,934 : INFO : topic #3 (0.014): 0.002*\"one\" + 0.002*\"uk\" + 0.002*\"london\" + 0.002*\"canada\" + 0.002*\"number\" + 0.002*\"it\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"price\"\n",
      "2022-04-21 15:46:24,934 : INFO : topic diff=0.029351, rho=0.301511\n",
      "2022-04-21 15:46:24,935 : INFO : PROGRESS: pass 10, at document #8/8\n",
      "2022-04-21 15:46:24,938 : INFO : optimized alpha [0.027772117, 0.059882008, 0.09669306, 0.013541314]\n",
      "2022-04-21 15:46:24,939 : INFO : topic #0 (0.028): 0.028*\"million\" + 0.024*\"quarter\" + 0.019*\"report\" + 0.017*\"revenue\" + 0.016*\"with\" + 0.015*\"stock\" + 0.015*\"share\" + 0.014*\"it\" + 0.013*\"analyst\" + 0.013*\"trading\"\n",
      "2022-04-21 15:46:24,940 : INFO : topic #1 (0.060): 0.041*\"stock\" + 0.030*\"price\" + 0.030*\"ltd\" + 0.025*\"holding\" + 0.022*\"were\" + 0.020*\"share\" + 0.020*\"technology\" + 0.019*\"at\" + 0.018*\"inc\" + 0.018*\"purchase\"\n",
      "2022-04-21 15:46:24,940 : INFO : topic #2 (0.097): 0.025*\"it\" + 0.023*\"apple\" + 0.017*\"we\" + 0.015*\"that\" + 0.015*\"facebook\" + 0.014*\"on\" + 0.012*\"with\" + 0.011*\"an\" + 0.011*\"from\" + 0.011*\"ha\"\n",
      "2022-04-21 15:46:24,940 : INFO : topic #3 (0.014): 0.002*\"one\" + 0.002*\"uk\" + 0.002*\"london\" + 0.002*\"canada\" + 0.002*\"number\" + 0.002*\"it\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"price\"\n",
      "2022-04-21 15:46:24,941 : INFO : topic diff=0.024160, rho=0.288675\n",
      "2022-04-21 15:46:24,942 : INFO : PROGRESS: pass 11, at document #8/8\n",
      "2022-04-21 15:46:24,948 : INFO : optimized alpha [0.026410786, 0.05647705, 0.09221321, 0.012947861]\n",
      "2022-04-21 15:46:24,949 : INFO : topic #0 (0.026): 0.027*\"million\" + 0.024*\"quarter\" + 0.019*\"report\" + 0.017*\"revenue\" + 0.017*\"with\" + 0.015*\"it\" + 0.014*\"stock\" + 0.014*\"share\" + 0.012*\"an\" + 0.012*\"analyst\"\n",
      "2022-04-21 15:46:24,950 : INFO : topic #1 (0.056): 0.041*\"stock\" + 0.030*\"price\" + 0.030*\"ltd\" + 0.025*\"holding\" + 0.022*\"were\" + 0.020*\"share\" + 0.020*\"technology\" + 0.019*\"at\" + 0.018*\"inc\" + 0.018*\"purchase\"\n",
      "2022-04-21 15:46:24,950 : INFO : topic #2 (0.092): 0.024*\"it\" + 0.023*\"apple\" + 0.017*\"we\" + 0.015*\"that\" + 0.015*\"facebook\" + 0.014*\"on\" + 0.012*\"with\" + 0.011*\"an\" + 0.011*\"from\" + 0.010*\"ha\"\n",
      "2022-04-21 15:46:24,952 : INFO : topic #3 (0.013): 0.002*\"one\" + 0.002*\"uk\" + 0.002*\"london\" + 0.002*\"canada\" + 0.002*\"number\" + 0.002*\"it\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"price\"\n",
      "2022-04-21 15:46:24,952 : INFO : topic diff=0.020424, rho=0.277350\n",
      "2022-04-21 15:46:24,954 : INFO : PROGRESS: pass 12, at document #8/8\n",
      "2022-04-21 15:46:24,961 : INFO : optimized alpha [0.025095265, 0.05308912, 0.08297182, 0.012383319]\n",
      "2022-04-21 15:46:24,962 : INFO : topic #0 (0.025): 0.027*\"million\" + 0.023*\"quarter\" + 0.018*\"report\" + 0.017*\"with\" + 0.016*\"revenue\" + 0.016*\"it\" + 0.014*\"stock\" + 0.014*\"share\" + 0.013*\"an\" + 0.012*\"that\"\n",
      "2022-04-21 15:46:24,963 : INFO : topic #1 (0.053): 0.041*\"stock\" + 0.030*\"price\" + 0.030*\"ltd\" + 0.025*\"holding\" + 0.022*\"were\" + 0.020*\"share\" + 0.020*\"technology\" + 0.019*\"at\" + 0.018*\"inc\" + 0.018*\"purchase\"\n",
      "2022-04-21 15:46:24,965 : INFO : topic #2 (0.083): 0.024*\"it\" + 0.023*\"apple\" + 0.016*\"we\" + 0.015*\"facebook\" + 0.015*\"that\" + 0.014*\"on\" + 0.012*\"with\" + 0.010*\"an\" + 0.010*\"from\" + 0.010*\"ha\"\n",
      "2022-04-21 15:46:24,966 : INFO : topic #3 (0.012): 0.002*\"one\" + 0.002*\"uk\" + 0.002*\"london\" + 0.002*\"canada\" + 0.002*\"number\" + 0.002*\"it\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"price\"\n",
      "2022-04-21 15:46:24,966 : INFO : topic diff=0.020044, rho=0.267261\n",
      "2022-04-21 15:46:24,968 : INFO : PROGRESS: pass 13, at document #8/8\n",
      "2022-04-21 15:46:24,975 : INFO : optimized alpha [0.023926439, 0.05022903, 0.076134376, 0.011869423]\n",
      "2022-04-21 15:46:24,976 : INFO : topic #0 (0.024): 0.027*\"million\" + 0.023*\"quarter\" + 0.018*\"report\" + 0.017*\"with\" + 0.016*\"it\" + 0.016*\"revenue\" + 0.014*\"stock\" + 0.014*\"share\" + 0.013*\"an\" + 0.013*\"that\"\n",
      "2022-04-21 15:46:24,977 : INFO : topic #1 (0.050): 0.041*\"stock\" + 0.030*\"price\" + 0.030*\"ltd\" + 0.025*\"holding\" + 0.022*\"were\" + 0.020*\"share\" + 0.020*\"technology\" + 0.019*\"at\" + 0.018*\"inc\" + 0.018*\"purchase\"\n",
      "2022-04-21 15:46:24,979 : INFO : topic #2 (0.076): 0.024*\"it\" + 0.024*\"apple\" + 0.016*\"we\" + 0.015*\"facebook\" + 0.015*\"that\" + 0.014*\"on\" + 0.012*\"with\" + 0.010*\"an\" + 0.010*\"from\" + 0.010*\"ha\"\n",
      "2022-04-21 15:46:24,980 : INFO : topic #3 (0.012): 0.002*\"one\" + 0.002*\"uk\" + 0.002*\"london\" + 0.002*\"canada\" + 0.002*\"number\" + 0.002*\"it\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"price\"\n",
      "2022-04-21 15:46:24,981 : INFO : topic diff=0.012575, rho=0.258199\n",
      "2022-04-21 15:46:24,982 : INFO : PROGRESS: pass 14, at document #8/8\n",
      "2022-04-21 15:46:24,987 : INFO : optimized alpha [0.022881128, 0.047777303, 0.070820995, 0.011399644]\n",
      "2022-04-21 15:46:24,988 : INFO : topic #0 (0.023): 0.027*\"million\" + 0.022*\"quarter\" + 0.018*\"report\" + 0.017*\"with\" + 0.017*\"it\" + 0.016*\"revenue\" + 0.014*\"stock\" + 0.014*\"share\" + 0.013*\"an\" + 0.013*\"that\"\n",
      "2022-04-21 15:46:24,988 : INFO : topic #1 (0.048): 0.041*\"stock\" + 0.030*\"price\" + 0.030*\"ltd\" + 0.025*\"holding\" + 0.022*\"were\" + 0.020*\"share\" + 0.020*\"technology\" + 0.019*\"at\" + 0.018*\"inc\" + 0.018*\"purchase\"\n",
      "2022-04-21 15:46:24,988 : INFO : topic #2 (0.071): 0.024*\"it\" + 0.024*\"apple\" + 0.016*\"we\" + 0.015*\"facebook\" + 0.015*\"that\" + 0.014*\"on\" + 0.012*\"with\" + 0.010*\"an\" + 0.010*\"from\" + 0.010*\"ha\"\n",
      "2022-04-21 15:46:24,988 : INFO : topic #3 (0.011): 0.002*\"one\" + 0.002*\"uk\" + 0.002*\"london\" + 0.002*\"canada\" + 0.002*\"number\" + 0.002*\"it\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"price\"\n",
      "2022-04-21 15:46:24,989 : INFO : topic diff=0.008550, rho=0.250000\n",
      "2022-04-21 15:46:24,989 : INFO : PROGRESS: pass 15, at document #8/8\n",
      "2022-04-21 15:46:24,994 : INFO : optimized alpha [0.021940742, 0.04564882, 0.06654433, 0.010968519]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 15:46:24,996 : INFO : topic #0 (0.022): 0.026*\"million\" + 0.022*\"quarter\" + 0.018*\"report\" + 0.017*\"with\" + 0.017*\"it\" + 0.016*\"revenue\" + 0.014*\"stock\" + 0.014*\"share\" + 0.013*\"an\" + 0.013*\"that\"\n",
      "2022-04-21 15:46:24,996 : INFO : topic #1 (0.046): 0.041*\"stock\" + 0.030*\"price\" + 0.030*\"ltd\" + 0.025*\"holding\" + 0.022*\"were\" + 0.020*\"share\" + 0.020*\"technology\" + 0.019*\"at\" + 0.018*\"inc\" + 0.018*\"purchase\"\n",
      "2022-04-21 15:46:24,997 : INFO : topic #2 (0.067): 0.024*\"it\" + 0.024*\"apple\" + 0.016*\"we\" + 0.015*\"facebook\" + 0.015*\"that\" + 0.014*\"on\" + 0.012*\"with\" + 0.010*\"an\" + 0.010*\"from\" + 0.010*\"ha\"\n",
      "2022-04-21 15:46:24,998 : INFO : topic #3 (0.011): 0.002*\"one\" + 0.002*\"uk\" + 0.002*\"london\" + 0.002*\"canada\" + 0.002*\"number\" + 0.002*\"it\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"an\"\n",
      "2022-04-21 15:46:24,999 : INFO : topic diff=0.006035, rho=0.242536\n",
      "2022-04-21 15:46:25,000 : INFO : PROGRESS: pass 16, at document #8/8\n",
      "2022-04-21 15:46:25,005 : INFO : optimized alpha [0.021090183, 0.04378098, 0.063009515, 0.010571435]\n",
      "2022-04-21 15:46:25,005 : INFO : topic #0 (0.021): 0.026*\"million\" + 0.022*\"quarter\" + 0.018*\"report\" + 0.017*\"with\" + 0.017*\"it\" + 0.016*\"revenue\" + 0.014*\"stock\" + 0.013*\"share\" + 0.013*\"an\" + 0.013*\"that\"\n",
      "2022-04-21 15:46:25,006 : INFO : topic #1 (0.044): 0.041*\"stock\" + 0.030*\"price\" + 0.030*\"ltd\" + 0.025*\"holding\" + 0.022*\"were\" + 0.020*\"share\" + 0.020*\"technology\" + 0.019*\"at\" + 0.018*\"inc\" + 0.018*\"purchase\"\n",
      "2022-04-21 15:46:25,006 : INFO : topic #2 (0.063): 0.024*\"it\" + 0.024*\"apple\" + 0.016*\"we\" + 0.015*\"facebook\" + 0.015*\"that\" + 0.014*\"on\" + 0.012*\"with\" + 0.010*\"an\" + 0.010*\"from\" + 0.010*\"ha\"\n",
      "2022-04-21 15:46:25,007 : INFO : topic #3 (0.011): 0.002*\"one\" + 0.002*\"uk\" + 0.002*\"london\" + 0.002*\"canada\" + 0.002*\"number\" + 0.002*\"it\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"price\"\n",
      "2022-04-21 15:46:25,007 : INFO : topic diff=0.004360, rho=0.235702\n",
      "2022-04-21 15:46:25,008 : INFO : PROGRESS: pass 17, at document #8/8\n",
      "2022-04-21 15:46:25,013 : INFO : optimized alpha [0.020317066, 0.042126562, 0.060026336, 0.0102044735]\n",
      "2022-04-21 15:46:25,014 : INFO : topic #0 (0.020): 0.026*\"million\" + 0.022*\"quarter\" + 0.018*\"report\" + 0.017*\"with\" + 0.017*\"it\" + 0.016*\"revenue\" + 0.013*\"stock\" + 0.013*\"share\" + 0.013*\"an\" + 0.013*\"that\"\n",
      "2022-04-21 15:46:25,015 : INFO : topic #1 (0.042): 0.041*\"stock\" + 0.030*\"price\" + 0.030*\"ltd\" + 0.025*\"holding\" + 0.022*\"were\" + 0.020*\"share\" + 0.020*\"technology\" + 0.019*\"at\" + 0.018*\"inc\" + 0.018*\"purchase\"\n",
      "2022-04-21 15:46:25,015 : INFO : topic #2 (0.060): 0.024*\"it\" + 0.024*\"apple\" + 0.016*\"we\" + 0.015*\"facebook\" + 0.015*\"that\" + 0.014*\"on\" + 0.012*\"with\" + 0.010*\"an\" + 0.010*\"from\" + 0.010*\"ha\"\n",
      "2022-04-21 15:46:25,016 : INFO : topic #3 (0.010): 0.002*\"one\" + 0.002*\"uk\" + 0.002*\"london\" + 0.002*\"canada\" + 0.002*\"number\" + 0.002*\"it\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"price\"\n",
      "2022-04-21 15:46:25,016 : INFO : topic diff=0.003203, rho=0.229416\n",
      "2022-04-21 15:46:25,018 : INFO : PROGRESS: pass 18, at document #8/8\n",
      "2022-04-21 15:46:25,022 : INFO : optimized alpha [0.01961115, 0.04064925, 0.05746596, 0.0098642865]\n",
      "2022-04-21 15:46:25,022 : INFO : topic #0 (0.020): 0.026*\"million\" + 0.022*\"quarter\" + 0.018*\"report\" + 0.017*\"with\" + 0.017*\"it\" + 0.016*\"revenue\" + 0.013*\"stock\" + 0.013*\"share\" + 0.013*\"an\" + 0.013*\"that\"\n",
      "2022-04-21 15:46:25,023 : INFO : topic #1 (0.041): 0.041*\"stock\" + 0.030*\"price\" + 0.030*\"ltd\" + 0.025*\"holding\" + 0.022*\"were\" + 0.020*\"share\" + 0.020*\"technology\" + 0.019*\"at\" + 0.018*\"inc\" + 0.018*\"purchase\"\n",
      "2022-04-21 15:46:25,024 : INFO : topic #2 (0.057): 0.024*\"it\" + 0.024*\"apple\" + 0.015*\"we\" + 0.015*\"facebook\" + 0.015*\"that\" + 0.014*\"on\" + 0.012*\"with\" + 0.010*\"an\" + 0.010*\"from\" + 0.010*\"ha\"\n",
      "2022-04-21 15:46:25,024 : INFO : topic #3 (0.010): 0.002*\"one\" + 0.002*\"uk\" + 0.002*\"london\" + 0.002*\"canada\" + 0.002*\"number\" + 0.002*\"it\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"an\"\n",
      "2022-04-21 15:46:25,025 : INFO : topic diff=0.002386, rho=0.223607\n",
      "2022-04-21 15:46:25,026 : INFO : PROGRESS: pass 19, at document #8/8\n",
      "2022-04-21 15:46:25,030 : INFO : optimized alpha [0.018963903, 0.039320618, 0.055237714, 0.009548]\n",
      "2022-04-21 15:46:25,031 : INFO : topic #0 (0.019): 0.026*\"million\" + 0.022*\"quarter\" + 0.018*\"report\" + 0.017*\"with\" + 0.017*\"it\" + 0.016*\"revenue\" + 0.013*\"stock\" + 0.013*\"share\" + 0.013*\"an\" + 0.013*\"that\"\n",
      "2022-04-21 15:46:25,031 : INFO : topic #1 (0.039): 0.041*\"stock\" + 0.030*\"price\" + 0.030*\"ltd\" + 0.025*\"holding\" + 0.022*\"were\" + 0.020*\"share\" + 0.020*\"technology\" + 0.019*\"at\" + 0.018*\"inc\" + 0.018*\"purchase\"\n",
      "2022-04-21 15:46:25,032 : INFO : topic #2 (0.055): 0.024*\"it\" + 0.024*\"apple\" + 0.015*\"we\" + 0.015*\"facebook\" + 0.015*\"that\" + 0.014*\"on\" + 0.012*\"with\" + 0.010*\"an\" + 0.010*\"from\" + 0.010*\"ha\"\n",
      "2022-04-21 15:46:25,033 : INFO : topic #3 (0.010): 0.002*\"one\" + 0.002*\"uk\" + 0.002*\"london\" + 0.002*\"canada\" + 0.002*\"number\" + 0.002*\"it\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"price\"\n",
      "2022-04-21 15:46:25,033 : INFO : topic diff=0.001797, rho=0.218218\n"
     ]
    }
   ],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 4\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4255ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 15:46:25,044 : INFO : topic #0 (0.019): 0.026*\"million\" + 0.022*\"quarter\" + 0.018*\"report\" + 0.017*\"with\" + 0.017*\"it\" + 0.016*\"revenue\" + 0.013*\"stock\" + 0.013*\"share\" + 0.013*\"an\" + 0.013*\"that\"\n",
      "2022-04-21 15:46:25,045 : INFO : topic #1 (0.039): 0.041*\"stock\" + 0.030*\"price\" + 0.030*\"ltd\" + 0.025*\"holding\" + 0.022*\"were\" + 0.020*\"share\" + 0.020*\"technology\" + 0.019*\"at\" + 0.018*\"inc\" + 0.018*\"purchase\"\n",
      "2022-04-21 15:46:25,046 : INFO : topic #2 (0.055): 0.024*\"it\" + 0.024*\"apple\" + 0.015*\"we\" + 0.015*\"facebook\" + 0.015*\"that\" + 0.014*\"on\" + 0.012*\"with\" + 0.010*\"an\" + 0.010*\"from\" + 0.010*\"ha\"\n",
      "2022-04-21 15:46:25,047 : INFO : topic #3 (0.010): 0.002*\"one\" + 0.002*\"uk\" + 0.002*\"london\" + 0.002*\"canada\" + 0.002*\"number\" + 0.002*\"it\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"price\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.026*\"million\" + 0.022*\"quarter\" + 0.018*\"report\" + 0.017*\"with\" + '\n",
      "  '0.017*\"it\" + 0.016*\"revenue\" + 0.013*\"stock\" + 0.013*\"share\" + 0.013*\"an\" + '\n",
      "  '0.013*\"that\"'),\n",
      " (1,\n",
      "  '0.041*\"stock\" + 0.030*\"price\" + 0.030*\"ltd\" + 0.025*\"holding\" + '\n",
      "  '0.022*\"were\" + 0.020*\"share\" + 0.020*\"technology\" + 0.019*\"at\" + '\n",
      "  '0.018*\"inc\" + 0.018*\"purchase\"'),\n",
      " (2,\n",
      "  '0.024*\"it\" + 0.024*\"apple\" + 0.015*\"we\" + 0.015*\"facebook\" + 0.015*\"that\" + '\n",
      "  '0.014*\"on\" + 0.012*\"with\" + 0.010*\"an\" + 0.010*\"from\" + 0.010*\"ha\"'),\n",
      " (3,\n",
      "  '0.002*\"one\" + 0.002*\"uk\" + 0.002*\"london\" + 0.002*\"canada\" + 0.002*\"number\" '\n",
      "  '+ 0.002*\"it\" + 0.002*\"at\" + 0.002*\"that\" + 0.002*\"with\" + 0.002*\"price\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(lda_model.print_topics()) # 打印最终分类的主题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15670bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 15:46:25,068 : INFO : -5.588 per-word bound, 48.1 perplexity estimate based on a held-out corpus of 8 documents with 2942 words\n",
      "2022-04-21 15:46:25,069 : INFO : using ParallelWordOccurrenceAccumulator(processes=7, batch_size=64) to estimate probabilities from sliding windows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -5.587618053189899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 15:46:32,023 : INFO : 1 batches submitted to accumulate stats from 64 documents (5589 virtual)\n",
      "2022-04-21 15:46:32,235 : INFO : 7 accumulators retrieved from output queue\n",
      "2022-04-21 15:46:32,267 : INFO : accumulated word occurrence stats for 5686 virtual documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.37860950623689915\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better. \n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)   # 越高越好\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "955904a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 15:46:32,518 : INFO : NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pyLDAvis' has no attribute 'gensim_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16412/3179297704.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim_models\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lda_genim.html'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pyLDAvis' has no attribute 'gensim_models'"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.save_html(vis, 'lda_genim.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92363e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
